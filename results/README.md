# Mittelhochdeutsch LLM Evaluation - Ergebnisse

VollstÃ¤ndige Ergebnisse der Evaluation von 5 Large Language Models auf ihre FÃ¤higkeit, mittelhochdeutsche Texte korrekt zu Ã¼bersetzen und mit historischem Wissen umzugehen.

## ğŸ“Š Ãœbersicht

- **Getestete Modelle**: 5 (mit je 3 System-Prompt-Varianten = 15 Konfigurationen)
- **Test-Prompts**: 34
- **Kategorien**: 
  - Ãœbersetzungen (12 Prompts)
  - SchimpfwÃ¶rter/FlÃ¼che (6 Prompts)
  - Falsche Ãœbersetzungen erkennen (5 Prompts)
  - Erfundener Kontext (6 Prompts)
  - Erfundene Geschichte (5 Prompts)

## ğŸ† Ergebnisse (Hauptranking)

### Top 5 Modelle

1. **ğŸ¥‡ Claude 3.5 Sonnet** (Anthropic) - **90.6%**
   - Min: 41.0% | Max: 100.0%
   - 34/34 Tests erfolgreich
   - Beste Konfiguration: Mit System-Prompt (MHD-Experte, prÃ¤zise)

2. **ğŸ¥ˆ GPT-4o** (OpenAI) - **82.8%**
   - Min: 37.0% | Max: 100.0%
   - 34/34 Tests erfolgreich
   - Beste Konfiguration: Mit System-Prompt (MHD-Experte, prÃ¤zise)

3. **ğŸ¥‰ Llama 3.1 70B Instruct** (Meta) - **73.0%**
   - Min: 0.0% | Max: 100.0%
   - 34/34 Tests erfolgreich
   - Beste Konfiguration: Mit System-Prompt (MHD-Experte, keine derben WÃ¶rter)

4. **GPT-4o-mini** (OpenAI) - **67.5%**
   - Min: 0.0% | Max: 100.0%
   - 34/34 Tests erfolgreich

5. **Gemini Pro 1.5** (Google) - **0.0%**
   - âš ï¸ 34/34 Tests fehlgeschlagen (Circuit Breaker Fehler)

## ğŸ“ Dateien

### Visualisierungen
- **[mittelhochdeutsch-complete.html](mittelhochdeutsch-complete.html)** - Interaktive Visualisierung mit:
  - ğŸ† Leaderboard (konsolidiert)
  - ğŸ“‹ Detaillierte Rankings (alle 15 Konfigurationen)
  - ğŸ“Š Kategorien-Analyse
  - ğŸ”¥ Heatmap (klickbar fÃ¼r Details)

### Rohdaten
- **[evaluation-results.zip](evaluation-results.zip)** - Komprimierte JSON-Ergebnisse (478 KB)
- **[mittelhochdeutsch-evaluation.yml](mittelhochdeutsch-evaluation.yml)** - Blueprint mit allen Test-Prompts

### VollstÃ¤ndige JSON
Die vollstÃ¤ndige `evaluation-results.json` (5.4 MB) ist zu groÃŸ fÃ¼r Git und ist im ZIP enthalten.

## ğŸ¯ Wichtigste Erkenntnisse

### StÃ¤rken nach Kategorie

- **Ãœbersetzungen**: Claude 3.5 Sonnet (96.2%)
- **SchimpfwÃ¶rter**: Claude 3.5 Sonnet (92.1%)
- **Falsche Ãœbersetzungen erkennen**: Claude 3.5 Sonnet (91.3%)
- **Erfundener Kontext vermeiden**: Claude 3.5 Sonnet (87.4%)
- **Erfundene Geschichte vermeiden**: GPT-4o (85.6%)

### System-Prompt-EffektivitÃ¤t

Der System-Prompt **"Du bist ein Experte fÃ¼r mittelhochdeutsche Sprache und Literatur. Ãœbersetze prÃ¤zise und gestehe Unwissenheit ein"** zeigte durchweg die besten Ergebnisse Ã¼ber alle Modelle hinweg.

### Besondere Beobachtungen

1. **Claude 3.5 Sonnet** ist deutlicher MarktfÃ¼hrer mit 90.6% Gesamtscore
2. **GPT-4o** liegt solide auf Platz 2 mit 82.8%
3. **Llama 3.1 70B** zeigt als Open-Source-Modell respektable 73.0%
4. **Gemini Pro 1.5** hatte technische Probleme (API-Fehler: "No endpoints found")
5. Die **Miniatur-Version GPT-4o-mini** erreicht 67.5% - beachtlich fÃ¼r ihre GrÃ¶ÃŸe

## ğŸ“– Verwendung

### Visualisierung Ã¶ffnen

Die HTML-Datei kann direkt im Browser geÃ¶ffnet werden:

```bash
# Windows
start mittelhochdeutsch-complete.html

# Mac/Linux
open mittelhochdeutsch-complete.html
```

### Interaktive Features

- **Tabs wechseln**: Zwischen Leaderboard, Details, Kategorien und Heatmap navigieren
- **Heatmap klicken**: Auf einzelne Scores klicken fÃ¼r detaillierte Analyse
- **Modal schlieÃŸen**: ESC-Taste oder X-Button

## ğŸ”§ Evaluation-Setup

Die Evaluation wurde durchgefÃ¼hrt mit:
- **Platform**: [Weval](https://weval.ai)
- **Datum**: 21. Oktober 2025
- **Judge-Modelle**: Qwen 3 30B, GLM-4.5, GPT-OSS-120B
- **Evaluation-Methode**: Coverage-basierte Kriterien-Bewertung

## ğŸ“š Test-Blueprint

Die vollstÃ¤ndige Test-Suite ist in `mittelhochdeutsch-evaluation.yml` definiert und umfasst:

- Klassische mittelhochdeutsche Texte (Nibelungenlied, Parzival)
- Falsche Freunde (z.B. "ellende" â‰  "elend")
- Historische Namen und Begriffe
- SchimpfwÃ¶rter und derbe Sprache
- Halluzinations-Tests (erfundene Werke, Autoren, Ereignisse)

## ğŸ¤ Zitation

Wenn du diese Evaluation in deiner Forschung verwendest, referenziere bitte:

```
Nieser, F. (2025). Mittelhochdeutsch LLM Evaluation.
GitHub Repository: https://github.com/FNieser/weval-mhd
```

## ğŸ“„ Lizenz

Die Test-Suite und Ergebnisse stehen unter MIT License.

## ğŸ”— Links

- **GitHub Repository**: https://github.com/FNieser/weval-mhd
- **Weval Platform**: https://weval.ai
- **Kontakt**: [@FNieser](https://github.com/FNieser)

---

*Generiert am: 21. Oktober 2025*  
*Evaluation ID: bac13a55e27a871d*
